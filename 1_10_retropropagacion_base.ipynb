{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daniel Isai Catonga Tecla\n",
    "# Retropropagación a un paso\n",
    "Ejercicio (4 puntos posibles)\n",
    "\n",
    "La retropropagación nos brinda un mecanismo para entrenar redes de más de una capa. Su funcionamiento traslada el error en la predicción hacia atŕas usando la derivada parcial de dicho error con respecto de cada uno de los pesos. Es decir,\n",
    "\n",
    "$$\n",
    "\t\t\\frac{\\partial E}{\\partial w_{ho}} = \\frac{\\partial E}{\\partial h_o} \\frac{\\partial h_o}{\\partial w_{ho}}  =   \\frac{\\partial E}{\\partial h_o} y_h\n",
    "$$\n",
    "\n",
    "donde el subíndice $o$ indica output y $h$ hidden\n",
    "\n",
    "En este primer programa implementaremos el algoritmo de retropropagación para determinar los incrementos que debe tener cada capa de la red. En un siguiente ejercicio realizaremos todo el proceso de retropropagación.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/cv2course_intro_nn/blob/master/06_retropropagacion_base.ipynb)\n",
    "\n",
    "@juan1rving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos paquetes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos funciones útiles\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una arquitectura de red 3x2x1\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pase frontal\n",
    "\n",
    "Definimos el comportamiento de la red en el pase frontal.\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(WX)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48497343084992534\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pase hacia atrás\n",
    "\n",
    "Calcula el cambio que deben de tener los pesos de acuerdo al error. Recordemos que para la capa de salida el término de error que se aplica es:\n",
    "\n",
    "$$\n",
    "\\delta_o = (y-\\hat{y})f_o'(h)\n",
    "$$\n",
    "\n",
    "Y para la capa oculta: \n",
    "\n",
    "$$\n",
    "\\delta_h = \\delta_o w_{h,o} f_h'(h_h)\n",
    "$$\n",
    "\n",
    "este último término de error es diferente para cada neurona con índice $h$ de la capa oculta. Es decir tendremos tantas h como neuronas tenga la capa oculta. Nota que aunque el ejercicio se puede resolver usando vectores e índices, lo implementaremos usando notación matricial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.11502656915007464\n",
      "Delta Output: 0.02712989695909555\n",
      "Delta Hidden: [ 0.00066857 -0.00193079]\n"
     ]
    }
   ],
   "source": [
    "#TODO (1 punto): completa el código\n",
    "\n",
    "## Backward pass\n",
    "## TODO: Calcula el error residual\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calcula el término de error para la capa de salida\n",
    "del_err_output = error * sigmoid_prime(output)\n",
    "\n",
    "# TODO: Calcula el término de error para la capa oculta\n",
    "del_err_hidden = del_err_output * np.multiply(weights_hidden_output, sigmoid_prime(hidden_layer_input))\n",
    "\n",
    "print(\"Error:\", error)\n",
    "print(\"Delta Output:\", del_err_output)\n",
    "print(\"Delta Hidden:\", del_err_hidden)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que sabemos los términos de error que aplican en cada capa, procedemos a determinar los incrementos en cada capa. \n",
    "\n",
    "\n",
    "Para la capa de salida\n",
    "$$\n",
    "\\Delta w_{h,o} = \\eta \\delta_o \\hat{y}_h \n",
    "$$\n",
    "\n",
    "Para la capa oculta\n",
    "$$\\Delta w_{i,h} = \\eta \\delta_h x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremento de los pesos oculta a salida:\n",
      "[0.0015185  0.00104989]\n",
      "Incremento de los pesos de entrada a oculta:\n",
      "[[ 3.34286831e-05 -9.65394849e-05]\n",
      " [ 6.68573663e-06 -1.93078970e-05]\n",
      " [-1.33714733e-05  3.86157940e-05]]\n",
      "Nuevos pesos oculta a salida:\n",
      "[ 0.10303699 -0.29790022]\n",
      "Nuevos pesos de entrada a oculta:\n",
      "[[ 0.50006686 -0.60019308]\n",
      " [ 0.10001337 -0.20003862]\n",
      " [ 0.09997326  0.70007723]]\n"
     ]
    }
   ],
   "source": [
    "#TODO (2 puntos): Completa el código\n",
    "\n",
    "# TODO: Propon un valor para la tasa de aprendizaje diferente a la original\n",
    "learnrate = 0.1\n",
    "\n",
    "# TODO: Calcular el incremento en cada peso de la capa oculta a la salida\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output\n",
    "\n",
    "# TODO: Calcular el incremento en la capa de entrada a la oculta\n",
    "delta_w_i_h = learnrate * (del_err_hidden * x[:,None])\n",
    "\n",
    "# TODO: Actualiza los pesos\n",
    "weights_hidden_output += delta_w_h_o\n",
    "weights_input_hidden += delta_w_i_h\n",
    "\n",
    "print('Incremento de los pesos oculta a salida:')\n",
    "print(delta_w_h_o)\n",
    "\n",
    "print('Incremento de los pesos de entrada a oculta:')\n",
    "print(delta_w_i_h)\n",
    "\n",
    "print('Nuevos pesos oculta a salida:')\n",
    "print(weights_hidden_output)\n",
    "\n",
    "print('Nuevos pesos de entrada a oculta:')\n",
    "print(weights_input_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contesta a las siguientes preguntas (1 punto):\n",
    "\n",
    "¿La tasa de aprendizaje disminuye en cada capa?\n",
    "\n",
    "No, la tasa de aprendizaje (learnrate) es la misma para todas las capas. Se aplica por igual tanto a los pesos entre la capa de entrada y la oculta, como entre la capa oculta y la de salida.\n",
    "\n",
    "¿Son del mismo tamaño las matrices de los incrementos que las matrices de los pesos?\n",
    "\n",
    "Sí, los incrementos tienen exactamente las mismas dimensiones que los pesos, para que puedan sumarse directamente al momento de incrementar o actualizar los pesos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
